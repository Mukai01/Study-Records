{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f87b7af",
   "metadata": {},
   "source": [
    "# OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0d865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bbfb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04039582  0.00243626 -0.03664973  0.00888709]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)  # カートの位置、速度、棒の角度、角速度\n",
    "\n",
    "action_space = env.action_space \n",
    "print(action_space)  # 行動の次元数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb03d99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0403471  -0.19214144 -0.03647199  0.28978503]\n"
     ]
    }
   ],
   "source": [
    "action = 0  # or 1\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407cea6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c97189",
   "metadata": {},
   "source": [
    "# DQNのコア技術"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b5f01",
   "metadata": {},
   "source": [
    "### 経験再生\n",
    "今の経験と、1秒後の経験の間に相関関係があるので、うまく学習できない。経験をいったんバッファに保存して、ランダムに取り出して学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a82a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size) # 最大のサイズを指定\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def get_batch(self):\n",
    "        data = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        state = np.stack([x[0] for x in data])\n",
    "        action = np.array([x[1] for x in data])\n",
    "        reward = np.array([x[2] for x in data])\n",
    "        next_state = np.stack([x[3] for x in data])\n",
    "        done = np.array([x[4] for x in data]).astype(np.int)\n",
    "        return state, action, reward, next_state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6397c5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n",
      "(32,)\n",
      "(32,)\n",
      "(32, 4)\n",
      "(32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c076064769b9>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  done = np.array([x[4] for x in data]).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "replay_buffer = ReplayBuffer(buffer_size=10000, batch_size=32)\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "for episode in range(10):\n",
    "    while not done:\n",
    "        action = 0\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "state, action, reward, next_state, done = replay_buffer.get_batch()\n",
    "print(state.shape)  # (32, 4)\n",
    "print(action.shape)  # (32,)\n",
    "print(reward.shape)  # (32,)\n",
    "print(next_state.shape)  # (32, 4)\n",
    "print(done.shape)  # (32,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab800d",
   "metadata": {},
   "source": [
    "### ターゲットネットワーク\n",
    "教師学習とは違い、正解ラベルがQ関数を更新すると変動する為適用する。モデルを用意して、定期的に重みを同期して、このモデルを用いてターゲットを計算する。つまり、ターゲットを一定期間固定するためのテクニック。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deef5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dezero import Model\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "class QNet(Model):\n",
    "    def __init__(self, action_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(128)\n",
    "        self.l2 = L.Linear(128)\n",
    "        self.l3 = L.Linear(action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.0005\n",
    "        self.epsilon = 0.05\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 32\n",
    "        self.action_size = 2\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "        self.qnet = QNet(self.action_size)\n",
    "        self.qnet_target = QNet(self.action_size)\n",
    "        self.optimizer = optimizers.Adam(self.lr)\n",
    "        self.optimizer.setup(self.qnet)  # qnetだけを更新する\n",
    "\n",
    "    def sync_qnet(self):\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            state = state[np.newaxis, :]  # バッチの次元を追加\n",
    "            qs = self.qnet(state)\n",
    "            return qs.data.argmax()\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.get_batch()\n",
    "        qs = self.qnet(state)  # 32x4のstateを与える、qsは32x2(2つの行動のq関数の値)\n",
    "        q = qs[np.arange(self.batch_size), action]  # 取った行動のq関数の値を取得\n",
    "\n",
    "        next_qs = self.qnet_target(next_state)  # qnet_targetを使って更新\n",
    "        next_q = next_qs.max(axis=1)\n",
    "        next_q.unchain()\n",
    "        td_target = reward + (1 - done) * self.gamma * next_q  # doneをマスクとしてターゲットを計算\n",
    "\n",
    "        loss = F.mean_squared_error(q, td_target)\n",
    "\n",
    "        self.qnet.cleargrads()\n",
    "        loss.backward()\n",
    "        self.optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ff35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :0, total reward : 10.0, epsilon: 0.05\n",
      "episode :10, total reward : 10.0, epsilon: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c076064769b9>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  done = np.array([x[4] for x in data]).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :20, total reward : 12.0, epsilon: 0.05\n",
      "episode :30, total reward : 9.0, epsilon: 0.05\n",
      "episode :40, total reward : 9.0, epsilon: 0.05\n",
      "episode :50, total reward : 11.0, epsilon: 0.05\n",
      "episode :60, total reward : 13.0, epsilon: 0.05\n",
      "episode :70, total reward : 11.0, epsilon: 0.05\n",
      "episode :80, total reward : 13.0, epsilon: 0.05\n",
      "episode :90, total reward : 9.0, epsilon: 0.05\n",
      "episode :100, total reward : 10.0, epsilon: 0.05\n",
      "episode :110, total reward : 9.0, epsilon: 0.05\n",
      "episode :120, total reward : 9.0, epsilon: 0.05\n",
      "episode :130, total reward : 9.0, epsilon: 0.05\n",
      "episode :140, total reward : 60.0, epsilon: 0.05\n",
      "episode :150, total reward : 130.0, epsilon: 0.05\n",
      "episode :160, total reward : 91.0, epsilon: 0.05\n",
      "episode :170, total reward : 173.0, epsilon: 0.05\n",
      "episode :180, total reward : 197.0, epsilon: 0.05\n",
      "episode :190, total reward : 200.0, epsilon: 0.05\n",
      "episode :200, total reward : 200.0, epsilon: 0.05\n",
      "episode :210, total reward : 145.0, epsilon: 0.05\n",
      "episode :220, total reward : 170.0, epsilon: 0.05\n",
      "episode :230, total reward : 127.0, epsilon: 0.05\n",
      "episode :240, total reward : 147.0, epsilon: 0.05\n",
      "episode :250, total reward : 127.0, epsilon: 0.05\n",
      "episode :260, total reward : 129.0, epsilon: 0.05\n",
      "episode :270, total reward : 139.0, epsilon: 0.05\n",
      "episode :280, total reward : 130.0, epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "episodes = 300\n",
    "sync_interval = 20\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = DQNAgent()\n",
    "reward_log = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        sum_reward += reward\n",
    "\n",
    "    if episode % sync_interval == 0:\n",
    "        agent.sync_qnet() # 20episodeごとに同期\n",
    "\n",
    "    reward_log.append(sum_reward)\n",
    "    if episode % 10 == 0:\n",
    "        print(\"episode :{}, total reward : {}, epsilon: {}\".format(episode, sum_reward, agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5553f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = 0  # greedy policy\n",
    "state = env.reset()\n",
    "done = False\n",
    "sum_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state = next_state\n",
    "    sum_reward += reward\n",
    "    env.render()\n",
    "print('Total Reward:', sum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3426a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
